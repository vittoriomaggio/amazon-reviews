{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Tokeniz-stem.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"trqqpjWA9_1u","colab_type":"text"},"source":["# TOKENIZE - STEMMING\n","Questo file ha come obettivo quello di generare un nuovo file in cui è stata applicata la tokenizzazione e lo stemming."]},{"cell_type":"markdown","metadata":{"id":"OC4ZEPZC-qvq","colab_type":"text"},"source":["### Import packages"]},{"cell_type":"code","metadata":{"id":"KR-BfQiA6Tpr","colab_type":"code","colab":{}},"source":["#Import packages\n","import numpy as np\n","import pandas as pd\n","import string\n","import itertools\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from collections import Counter\n","from PIL import Image\n","from wordcloud import WordCloud\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRJwmqx4-ugh","colab_type":"text"},"source":["### Import document"]},{"cell_type":"code","metadata":{"id":"NgIHx_ct6Tpv","colab_type":"code","colab":{}},"source":["#lettura del csv\n","reviews = pd.read_csv(\"reviews_sentiment.csv\")\n","\n","# Campo per visualizzare i commenti lunghi\n","pd.set_option('display.max_colwidth', -1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLkVowXT_Do7","colab_type":"text"},"source":["### Creazione di una funzione per la regex"]},{"cell_type":"code","metadata":{"id":"tTRyMkqr6Tp2","colab_type":"code","colab":{}},"source":["import re\n","\n","regex_str = [\n","    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n","    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n","    r\"(?:[a-zA-Z][a-zA-Z-_]+[a-zA-Z])\", # words with -\n","    r\"(?:[a-zA-Z][a-zA-Z'])\", # words with '\n","]\n","\n","WORD = re.compile(r'('+'|'.join(regex_str)+')')\n","#WORD = re.compile(r'\\w+')\n","\n","def regTokenize(text):\n","    words = WORD.findall(text)\n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgbvj5Hr8xXy","colab_type":"text"},"source":["\n","### Tokenizzazione\n","Il primo passo comune nella PNL (Natural Language Processing) è la tokenizzazione essa prende un testo o un insieme di testo e lo suddivide in token, che sono in genere singole parole. "]},{"cell_type":"code","metadata":{"id":"1VFzDP3b6Tp6","colab_type":"code","colab":{}},"source":["#Tokenizzazione\n","# Per ogni review all'interno del campo \"body\", applica la funzione regTokenize\n","i = 0\n","for x in reviews[\"body\"]:\n","    token = regTokenize(x)\n","    reviews[\"body\"][i] = token\n","    i = i+1\n","print(reviews[\"body\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"APQYgHdU6Tp9","colab_type":"code","colab":{}},"source":["#Da uppercase a lowercase\n","lowered_text = reviews[\"body\"].apply(lambda x: [item.lower() for item in x])\n","lowered_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBd2twpm6TqA","colab_type":"code","colab":{}},"source":["stop = stopwords.words(\"italian\")\n","stop = set(stop)\n","stop.add(\"l'\")\n","stop.add(\"c'\")\n","\n","f = open(\"Stopwords.txt\",\"r\")\n","lines = f.readlines()\n","result = []\n","for x in lines:\n","    result.append(x)\n","f.close()\n","\n","for i in result:\n","    val = re.sub('[^A-Za-z0-9]+', '', i)\n","    stop.add(val)\n","\n","#Tolgo le stopwords    \n","stopword_removed_text = lowered_text.apply(lambda x: [item for item in x if item not in stop])\n","stopword_removed_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"78ImR4rr6TqF","colab_type":"code","colab":{}},"source":["#Tolgo le cifre numeriche\n","preprocessed_text = stopword_removed_text.apply(lambda x: [item for item in x if not item.isdigit()]) \n","preprocessed_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dyWBaAEU6TqI","colab_type":"code","colab":{}},"source":["from nltk.stem.snowball import SnowballStemmer\n","ita_stemmer = SnowballStemmer(\"italian\")\n","\n","tokenized_stemming = preprocessed_text.apply(lambda x: [ita_stemmer.stem(item) for item in x])\n","tokenized_stemming"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1j4vzO8_6TqL","colab_type":"code","colab":{}},"source":["tokenizedDataset = pd.DataFrame()\n","tokenizedDataset[\"tokens_stemming\"] = tokenized_stemming\n","tokenizedDataset[\"tokens\"] = preprocessed_text\n","tokenizedDataset.to_csv(r'.\\tokenizedDataset.csv')"],"execution_count":0,"outputs":[]}]}